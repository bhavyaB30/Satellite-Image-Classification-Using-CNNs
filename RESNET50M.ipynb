{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UhuHfK8RDy7"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"eurosat/rgb\"\n",
        "dataset, info = tfds.load(dataset_name, split=\"train\", as_supervised=True, with_info=True)"
      ],
      "metadata": {
        "id": "9EnrMjb9RT6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_12th_per_class(dataset, num_classes, class_names):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    samples = {i: None for i in range(num_classes)}\n",
        "    counts = {i: 0 for i in range(num_classes)}\n",
        "\n",
        "    for image, label in dataset:\n",
        "        label_val = label.numpy()\n",
        "        counts[label_val] += 1\n",
        "\n",
        "        if counts[label_val] == 12:\n",
        "            samples[label_val] = (image, label)\n",
        "\n",
        "        if all(v is not None for v in samples.values()):\n",
        "            break\n",
        "\n",
        "    for i, (image, label) in enumerate(samples.values()):\n",
        "        plt.subplot(3, (num_classes + 2) // 3, i + 1)\n",
        "        plt.imshow(image.numpy())\n",
        "        plt.title(class_names[label.numpy()], fontsize=16)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_12th_per_class(dataset, len(class_names), class_names)"
      ],
      "metadata": {
        "id": "0N_0V-vhRT9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)"
      ],
      "metadata": {
        "id": "AlqWYa14RUAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = info.features[\"label\"].names\n",
        "print(\"EuroSAT Class Names:\")\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"{i}: {name}\")"
      ],
      "metadata": {
        "id": "1Jj_lw0YRUDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(dataset))\n",
        "\n",
        "tensor_shape = tf.shape(image)\n",
        "\n",
        "print(f\"Shape of image: {tensor_shape}\")"
      ],
      "metadata": {
        "id": "OjQoOAqzRUGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(dataset)\n",
        "\n",
        "print(f\"Number of samples in the train dataset: {train_size}\")"
      ],
      "metadata": {
        "id": "Pyt-IhwSRUKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.python.keras as k\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D ,Dropout\n",
        "from tensorflow.keras.initializers import random_uniform, glorot_uniform\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "zxNGwZ2-RUM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "-OjphNdoRUQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mNouH2YrRUTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAiLpVOURUWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwodgjNFG571"
      },
      "outputs": [],
      "source": [
        "def identity_block(X, f, filters, training=True, initializer=random_uniform):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    training -- True: Behave in training mode\n",
        "                False: Behave in inference mode\n",
        "    initializer -- to set up the initial weights of a layer. Equals to random uniform initializer\n",
        "\n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value.\n",
        "    X_shortcut = X\n",
        "    cache = []\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (1, 1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = Add()([X_shortcut, X])\n",
        "    X = X = Activation('relu')(X, training = training)\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nF61i_LLRUZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF_kCyTBJRnB"
      },
      "outputs": [],
      "source": [
        "def convolutional_block(X, f, filters, s = 2, training=True, initializer=glorot_uniform):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    s -- Integer, specifying the stride to be used\n",
        "    training -- True: Behave in training mode\n",
        "                False: Behave in inference mode\n",
        "    initializer -- to set up the initial weights of a layer. Equals to Glorot uniform initializer,\n",
        "                   also called Xavier uniform initializer.\n",
        "\n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "\n",
        "    # First component of main path glorot_uniform(seed=0)\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training=training)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "\n",
        "\n",
        "    # Second component of main path (≈3 lines)\n",
        "    X = Conv2D(F2, (f, f), strides = (1, 1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Dropout(0.2)(X)\n",
        "\n",
        "    # Third component of main path (≈2 lines)\n",
        "    X = Conv2D(F3, (1, 1), strides = (1, 1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training)\n",
        "\n",
        "    ##### SHORTCUT PATH #### (≈2 lines)\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s, s), padding = 'valid', kernel_initializer = initializer(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut, training = training)\n",
        "\n",
        "    # Final step: Add shortcut value to main path (Use this order [X, X_shortcut]), and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet50(input_shape = (64, 64, 3), classes = 10):\n",
        "    \"\"\"\n",
        "    Stage-wise implementation of the architecture of the popular ResNet50:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> FLATTEN -> DENSE\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape, dtype=\"float32\")\n",
        "    x = Conv2D(3, (1, 1), activation='relu')(X_input)\n",
        "\n",
        "\n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "\n",
        "   # Stage 3 (≈4 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "\n",
        "    # Stage 4 (≈6 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "\n",
        "    # Stage 5 (≈3 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "\n",
        "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    X = AveragePooling2D(pool_size = (2, 2), name = 'avg_pool')(X)\n",
        "\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "g3ZLeV6ISl2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = ResNet50(input_shape=(64, 64, 3), classes=10)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "H5Dz5nW_Sl5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4CbGcU3USl8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_size = tf.data.experimental.cardinality(dataset).numpy()\n",
        "train_size = int(0.7 * total_size)  # 70% for training\n",
        "val_size = int(0.1 * total_size)    # 10% for validation\n",
        "test_size = total_size - train_size - val_size  # 20% for testing"
      ],
      "metadata": {
        "id": "orQqsBbGSl-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 27000  # Use a large enough buffer size\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=False)"
      ],
      "metadata": {
        "id": "JOi9gf-dSmBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size).take(val_size)\n",
        "test_dataset = dataset.skip(train_size + val_size)"
      ],
      "metadata": {
        "id": "zKPo667mSmDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "yIRXa4-XSmF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.2),\n",
        "    RandomZoom(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "yqH8zY60VWwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "train_dataset = test_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "val_dataset = val_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))"
      ],
      "metadata": {
        "id": "L17cc2bgVXnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define Early Stopping Callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=5,          # Stop training if val_loss doesn't improve for 5 epochs\n",
        "    restore_best_weights=True  # Restore best model weights after stopping\n",
        ")"
      ],
      "metadata": {
        "id": "-abjNsIqSmJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=30,callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "e_c2ON19ThgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16.53, 11.69))\n",
        "ax1.plot(history.history['accuracy'])\n",
        "ax1.plot(history.history['val_accuracy'])\n",
        "ax1.set_xlabel('epoch')\n",
        "ax1.set_ylabel('accuracy')\n",
        "ax1.set_title('Accuracy over epoch')\n",
        "ax1.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "ax2.plot(history.history['loss'])\n",
        "ax2.plot(history.history['val_loss'])\n",
        "ax2.set_xlabel('epoch')\n",
        "ax2.set_ylabel('loss')\n",
        "ax2.set_title('Loss over epoch')\n",
        "ax2.legend(['Train', 'Test'], loc=\"upper right\")"
      ],
      "metadata": {
        "id": "RMwrklRjThi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/modelRESNET50XPLANABLEAI.h5')"
      ],
      "metadata": {
        "id": "pmKp8m6LThmX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}